{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_source = 'cv22_lab2_material/part3 - ImageStitching/1.jpg'\n",
    "img2_source = 'cv22_lab2_material/part3 - ImageStitching/2.jpg'\n",
    "img3_source = 'cv22_lab2_material/part3 - ImageStitching/3.jpg'\n",
    "img4_source = 'cv22_lab2_material/part3 - ImageStitching/4.jpg'\n",
    "img5_source = 'cv22_lab2_material/part3 - ImageStitching/5.jpg'\n",
    "img6_source = 'cv22_lab2_material/part3 - ImageStitching/6.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitchImages(img11, img22, result_img):\n",
    "    # Load our images\n",
    "    img1 = cv2.imread(img11)\n",
    "    img2 = cv2.imread(img22)\n",
    "\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    '''\n",
    "    cv2.imshow('img1_gray.jpg',img1_gray)\n",
    "    cv2.imshow('img2_gray.jpg',img2_gray)\n",
    "    '''\n",
    "    # Create our ORB detector and detect keypoints and descriptors\n",
    "    orb = cv2.ORB_create(nfeatures=2000)\n",
    "\n",
    "    # Find the key points and descriptors with ORB\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "    '''\n",
    "    cv2.imshow('img1_keypoints.jpg',cv2.drawKeypoints(img1, keypoints1, None, (255, 0, 255)))\n",
    "    cv2.imshow('img2_keypoints.jpg',cv2.drawKeypoints(img2, keypoints2, None, (255, 0, 255)))   \n",
    "    '''\n",
    "    # Create a BFMatcher object.\n",
    "    # It will find all of the matching keypoints on two images\n",
    "    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n",
    "\n",
    "    # Find matching points\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2,k=2)          \n",
    "    '''\n",
    "    print(keypoints1[0].pt)\n",
    "    print(keypoints1[0].size)\n",
    "    print(\"Descriptor of the first keypoint: \")\n",
    "    print(descriptors1[0])\n",
    "    '''\n",
    "           \n",
    "    all_matches = []\n",
    "    for m, n in matches:\n",
    "        all_matches.append(m)\n",
    "\n",
    "    draw_params = dict(matchColor=(0,255,0), singlePointColor=None, flags=2)\n",
    "    img3 = cv2.drawMatches(img1_gray, keypoints1, img2_gray, keypoints2, all_matches[:30], None, **draw_params)\n",
    "    '''\n",
    "    cv2.imshow('drawMatches.jpg',img3)   \n",
    "    '''\n",
    "    \n",
    "    # Finding the best matches\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.8 * n.distance:\n",
    "            good.append(m)\n",
    "    '''    \n",
    "    cv2.imshow('img1_drawKeypoints.jpg',cv2.drawKeypoints(img1, [keypoints1[m.queryIdx] for m in good], None, (255, 0, 255)))   \n",
    "    cv2.imshow('img1_drawKeypoints.jpg',cv2.drawKeypoints(img2, [keypoints2[m.trainIdx] for m in good], None, (255, 0, 255))) \n",
    "    '''\n",
    "\n",
    "    def warpImages(img1, img2, H):\n",
    "        rows1, cols1 = img1.shape[:2]\n",
    "        rows2, cols2 = img2.shape[:2]\n",
    "\n",
    "        list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)\n",
    "        temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n",
    "\n",
    "        # When we have established a homography we need to warp perspective\n",
    "        # Change field of view\n",
    "        list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n",
    "\n",
    "        list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n",
    "\n",
    "        [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n",
    "        [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n",
    "  \n",
    "        translation_dist = [-x_min,-y_min]\n",
    "  \n",
    "        H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n",
    "\n",
    "        output_img = cv2.warpPerspective(img2, H_translation.dot(H), (x_max-x_min, y_max-y_min))\n",
    "        output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n",
    "\n",
    "        return output_img\n",
    "    \n",
    "    MIN_MATCH_COUNT = 10\n",
    "\n",
    "    if len(good) > MIN_MATCH_COUNT:\n",
    "        # Convert keypoints to an argument for findHomography\n",
    "        src_pts = np.float32([ keypoints1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ keypoints2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "\n",
    "        # Establish a homography\n",
    "        M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    \n",
    "        result = warpImages(img2, img1, M)\n",
    "        \n",
    "        def trim(frame):\n",
    "            if not np.sum(frame[0]):\n",
    "                return trim(frame[1:])\n",
    "            if not np.sum(frame[-1]):\n",
    "                return trim(frame[:-2])\n",
    "            if not np.sum(frame[:,0]):\n",
    "                return trim(frame[:,1:])\n",
    "            if not np.sum(frame[:,-1]):\n",
    "                return trim(frame[:,:-2])\n",
    "            return frame\n",
    "        \n",
    "        cv2.imshow('result.jpg',trim(result))\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "    cv2.imwrite(result_img, trim(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img12='1_2.jpg'\n",
    "stitchImages(img1_source,img2_source,img12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "img123='1_2_3.jpg'\n",
    "stitchImages(img3_source,img12,img123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1234='1_2_3_4.jpg'\n",
    "stitchImages(img123,img4_source,img1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img12346='1_2_3_4_6.jpg'\n",
    "stitchImages(img1234,img6_source,img12346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img123456='1_2_3_4_5_6.jpg'\n",
    "stitchImages(img12346,img5_source,img123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
